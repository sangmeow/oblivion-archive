# 🤖 GPT (Generative Pre-trained Transformer)

> **사람처럼 대화하고, 글을 쓰고, 코드를 작성하는 인공지능 언어 모델**  
> OpenAI가 개발한 **GPT**는 방대한 텍스트 데이터를 학습해 문장을 이해하고 생성할 수 있습니다.

---

## 📌 1. 이름의 의미

| 구분            | 의미                                                     |
| --------------- | -------------------------------------------------------- |
| **Generative**  | 새로운 텍스트(문장, 문단, 코드 등) 생성 가능             |
| **Pre-trained** | 사전에 대규모 데이터로 학습 후 추가 학습 가능            |
| **Transformer** | Self-Attention 메커니즘 기반의 Transformer 아키텍처 사용 |

---

## ⚙ 2. 동작 원리

1. **사전 학습 (Pre-training)**
   - 웹 문서, 책, 뉴스 등 대규모 데이터로 언어 패턴 학습
2. **파인튜닝 (Fine-tuning)**
   - 특정 목적(채팅, 번역, 코딩 등)에 맞춰 추가 학습
3. **추론 (Inference)**
   - 입력된 문맥에 맞춰 다음 단어를 예측하며 문장 생성

---

## 🌟 3. 주요 특징

- 🧠 **문맥 이해** — 대화의 흐름과 주제 파악 가능
- ✍ **다목적 활용** — 채팅, 번역, 글쓰기, 코딩 등 가능
- 🗣 **자연스러운 표현** — 사람과 유사한 문장 생성
- 🔄 **멀티태스킹** — 하나의 모델로 다양한 작업 수행

---

## 📈 4. 버전 발전

| 버전      | 출시 연도      | 특징                                    |
| --------- | -------------- | --------------------------------------- |
| **GPT-1** | 2018           | 1.17억 파라미터, 개념 증명 수준         |
| **GPT-2** | 2019           | 15억 파라미터, 문장 생성 능력 대폭 향상 |
| **GPT-3** | 2020           | 1,750억 파라미터, 범용 언어 모델        |
| **GPT-4** | 2023           | 멀티모달(이미지+텍스트) 이해 가능       |
| **GPT-5** | 2024~2025 예상 | 더 높은 추론 능력과 정확성              |

---

## 💼 5. 활용 분야

- 📚 **글쓰기**: 기사, 소설, 블로그 작성
- 💻 **코딩**: 코드 생성, 디버깅, 문서화
- 🌍 **번역**: 다국어 번역 및 문맥 보정
- 🎓 **교육**: 질문 답변, 학습 자료 제작
- 📊 **비즈니스**: 보고서 작성, 데이터 분석, 고객 지원

---

## ✅ 6. 장점과 ⚠ 한계

### ✅ 장점

- 높은 생산성과 효율성
- 문맥 기반 자연스러운 응답
- 특정 주제 맞춤 재학습 가능

### ⚠ 한계

- **사실 오류 가능성**: 잘못된 정보도 자신 있게 말할 수 있음
- **편향(Bias)**: 학습 데이터의 한계
- **실시간 정보 제한**: 학습 이후 정보는 부족 (웹 연결 시 보완 가능)

---

## 📝 7. 간단한 예시

```text
사용자: GPT, 오늘 날씨 어때?
GPT: 저는 실시간 날씨를 확인할 수 없지만, 계절과 위치를 알려주시면 예측을 도와드릴 수 있습니다.
```
